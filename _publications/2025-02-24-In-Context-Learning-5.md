---
title: "Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers"
collection: publications
permalink: /publication/2025-02-24-In-Context-Learning-5
date: 2025-02-24
venue: 'Published in The Forty-First International Conference on Machine Learning (ICML 2024)'
author: Brian K Chen, Tianyang Hu, Hui Jin, Hwee Kuan Lee, Kenji Kawaguchi
paperurl: 'https://arxiv.org/pdf/2406.02847'
excerpt: 'We find a way to convert the prompts into the model weights by introducing an extra bias term into the attention module.'
---
<!-- excerpt: 'This paper is about the number 1. The number 2 is left for future work.' -->
<!-- citation: 'Jin, Hui, et al. &quot Noisy Subgraph Isomorphisms on Multiplex Networks. &quot <i>2019 IEEE International Conference on Big Data (Big Data)</i>. IEEE Computer Society, 2019.' -->

<!-- This paper is about the number 1. The number 2 is left for future work. -->

<!-- [Download paper here](https://www.math.ucla.edu/~bertozzi/papers/HuiJin-UCLA-Final-BIGDATA2019.pdf) -->
<!-- https://www.computer.org/csdl/proceedings-article/big-data/2019/09005645/1hJrOpGmfNm -->
<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->
