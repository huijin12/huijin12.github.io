---
title: "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training"
collection: publications
permalink: /publication/2025-02-24-How-Do-LLMs-Acquire-New-Knowledge
date: 2025-02-24
venue: 'Submitted to The 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025) '
author: Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen
paperurl: 'https://arxiv.org/pdf/2502.11196'
excerpt: 'We analyze how LLMs learn new knowledge through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. '
---
<!-- excerpt: 'This paper is about the number 1. The number 2 is left for future work.' -->
<!-- citation: 'Jin, Hui, et al. &quot Noisy Subgraph Isomorphisms on Multiplex Networks. &quot <i>2019 IEEE International Conference on Big Data (Big Data)</i>. IEEE Computer Society, 2019.' -->

<!-- This paper is about the number 1. The number 2 is left for future work. -->

<!-- [Download paper here](https://www.math.ucla.edu/~bertozzi/papers/HuiJin-UCLA-Final-BIGDATA2019.pdf) -->
<!-- https://www.computer.org/csdl/proceedings-article/big-data/2019/09005645/1hJrOpGmfNm -->
<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->
